{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf060d8-972d-4116-8ad8-157e075af1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Dropout,Flatten\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import pickle as pk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize, LineTokenizer , regexp_tokenize\n",
    "import string\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d527df34-5e2a-47b1-bfee-7ca56044abd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>1541.0</td>\n",
       "      <td>what's the matter with it?</td>\n",
       "      <td>it's too much like other flags.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3232</th>\n",
       "      <td>3231.0</td>\n",
       "      <td>what happened?</td>\n",
       "      <td>i had a car problem, so i went online.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>182.0</td>\n",
       "      <td>have you met the new girl?</td>\n",
       "      <td>no. have you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>1933.0</td>\n",
       "      <td>well, nobody else does, so why should i?</td>\n",
       "      <td>that's not the attitude of a good driver.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3233</th>\n",
       "      <td>3232.0</td>\n",
       "      <td>i had a car problem, so i went online.</td>\n",
       "      <td>did you find a solution?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>2925.0</td>\n",
       "      <td>well, i know how to flip hamburgers.</td>\n",
       "      <td>no one would hire you to flip hamburgers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>3702.0</td>\n",
       "      <td>they both said i have to live with it.</td>\n",
       "      <td>or you can stay off planes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>234.0</td>\n",
       "      <td>congratulations on your promotion.</td>\n",
       "      <td>thank you very much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>2300.0</td>\n",
       "      <td>he's a dirty old man.</td>\n",
       "      <td>what do you mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>3625.0</td>\n",
       "      <td>his doctor says his heart and lungs are strong...</td>\n",
       "      <td>maybe i should start smoking.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         num                                                  x   \n",
       "1542  1541.0                         what's the matter with it?  \\\n",
       "3232  3231.0                                     what happened?   \n",
       "183    182.0                         have you met the new girl?   \n",
       "1934  1933.0           well, nobody else does, so why should i?   \n",
       "3233  3232.0             i had a car problem, so i went online.   \n",
       "2926  2925.0               well, i know how to flip hamburgers.   \n",
       "3703  3702.0             they both said i have to live with it.   \n",
       "235    234.0                 congratulations on your promotion.   \n",
       "2301  2300.0                              he's a dirty old man.   \n",
       "3626  3625.0  his doctor says his heart and lungs are strong...   \n",
       "\n",
       "                                              y  \n",
       "1542            it's too much like other flags.  \n",
       "3232     i had a car problem, so i went online.  \n",
       "183                               no. have you?  \n",
       "1934  that's not the attitude of a good driver.  \n",
       "3233                   did you find a solution?  \n",
       "2926  no one would hire you to flip hamburgers.  \n",
       "3703                or you can stay off planes.  \n",
       "235                        thank you very much.  \n",
       "2301                          what do you mean?  \n",
       "3626              maybe i should start smoking.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv('Conversation.csv',names=['num','x','y'])\n",
    "dataset=dataset.drop(0)\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5d7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [\n",
    "    ['What is the capital of Egypt?', 'Cairo'],\n",
    "    ['What is the capital of Sudan?', 'Khartoum'],\n",
    "    ['What is the capital of Zambia?', 'Lusaka'],\n",
    "    ['What is the capital of Libya?', 'Tripoli'],\n",
    "    ['What is the capital of Somalia?', 'Mogadishu'],\n",
    "    ['What is the capital of Morocco?', 'Rabat'],\n",
    "    ['What is the capital of Kenya?', 'Nairobi'],\n",
    "    ['What is the capital of Nigeria?', 'Abuja'],\n",
    "    ['What is the capital of Tanzania?', 'Dodoma'],\n",
    "    ['How many players in Football (soccer) ?',\n",
    "     '11 players on each team (10 field players and 1 goalkeeper)'],\n",
    "    ['How many players in Volleyball ?',\n",
    "        '6 players on each team (3 front-row players and 3 back-row players)'],\n",
    "    ['How many players in Basketball ?', '5 players on each team'],\n",
    "    ['How many players in Tennis ?',\n",
    "        '1 player on each side of the court (singles) or 2 players on each side (doubles)'],\n",
    "    ['How many players in Baseball ?',\n",
    "        '9 players on each team (1 pitcher, 1 catcher, 4 infielders, and 3 outfielders)'],\n",
    "    ['How many players in Hockey ?',\n",
    "        '6 players on each team (1 goalie and 5 skaters)'],\n",
    "    ['How many players in Rugby ?', '15 players on each team'],\n",
    "    ['What is the ambulance number in Egypt ?', '123'],\n",
    "    ['What is the Police number in Egypt ?', '122'],\n",
    "    ['What is the number of Tourist Police in Egypt ?', '126'],\n",
    "    ['What is the Natural Gas Emergency number in Egypt ?', '129'],\n",
    "    ['What is the number of Traffic Police in Egypt ?', '128'],\n",
    "    ['What is the largest organ in the human body ?', 'The Skin'],\n",
    "    ['Diabetes affects the body’s ability to produce which hormone ?', 'Insulin'],\n",
    "    ['A cardiologist is a doctor who specialises in conditions affecting which organ ?', 'The heart'],\n",
    "    ['The fibula and tibia are bones found in which body part ?', 'The leg'],\n",
    "    ['How many more bones are babies born with than are part of the adult skeleton ?', '64'],\n",
    "    ['How many bones are there in the adult body ?', '206'],\n",
    "    ['What is the name of the membranes that cover the brain and spinal cord ?', 'Meninges'],\n",
    "    ['The father of medicine, Hippocrates, theorised that the secret to health was to maintain balance between blood, phlegm, black bile and yellow bile. What term did he use to describe these four most important fluids ?', 'The four humors'],\n",
    "    ['Which two parts of the body can take over the function of the spleen if it has to be removed?',\n",
    "        'The liver and bone marrow'],\n",
    "    ['What is the longest river in the world ?',\n",
    "        'Nile River: The Nile is approximately 6,650 km (4,132 miles) long and flows through eleven countries in Africa, including Egypt, Sudan, and Ethiopia'],\n",
    "    ['What is the biggest country in the world by land area ?',\n",
    "        'The biggest country in the world by land area is Russia'],\n",
    "    ['What is the normal human body temperature ?',\n",
    "        'The normal human body temperature is typically around 36.5 to 37.5 degrees Celsius (97.7 to 99.5 degrees Fahrenheit)'],\n",
    "    ['Tell me a small description about Natural Language Processing (NLP)?',   'Natural Language Processing (NLP) is a field of study in computer science and artificial intelligence that focuses on the interaction between human language and computers. It involves developing algorithms and computational models that enable computers to understand, interpret, and generate human language.'],\n",
    "    ['What is the goal of NLP?', 'The goal of NLP is to create computer programs that can effectively understand and process human language, including its nuances, context, and meaning. This requires overcoming a number of challenges, such as ambiguity, sarcasm, and cultural differences in language use.'],\n",
    "    ['What is the currency of Egypt?',\n",
    "     'Egyptian Pound (EGP)'],\n",
    "    ['What is the currency of Libya?',\n",
    "     'Libyan Dinar (LYD)'],\n",
    "    ['What is the currency of Morocco?',\n",
    "     'Moroccan Dirham (MAD)'],\n",
    "    ['What is the currency of Sudan?',\n",
    "     'Sudanese Pound (SDG)'],\n",
    "    ['What is the currency of United Kingdom (UK)?',\n",
    "     'Pound Sterling (GBP)'],\n",
    "    ['What is the currency of France?',\n",
    "     'Euro (EUR)'],\n",
    "    ['What is the currency of Germany?',\n",
    "     'Euro (EUR)'],\n",
    "    ['What is the currency of Turkey?',\n",
    "     'Turkish Lira (TRY)'],\n",
    "    ['What is the currency of United Arab Emirates (UAE)?',\n",
    "     'UAE Dirham (AED)'],\n",
    "    ['What is the currency of Palestin?',\n",
    "     'Jordanian Dinar (JOD)'],\n",
    "    ['What is the currency of Singapore?',\n",
    "     'Singapore Dollar (SGD)'],\n",
    "    ['What is Calculus?',\n",
    "     'The study of rates of change and the accumulation of infinitesimal quantities.'],\n",
    "    ['What is Algebraic geometry?',\n",
    "     'The study of algebraic varieties, which are sets of solutions to polynomial equations.'],\n",
    "    ['What is Combinatorics?',\n",
    "     'The study of counting, arrangements, and permutations of objects.'],\n",
    "    ['What is Topology?',\n",
    "     'The study of the properties of shapes and spaces that are preserved under continuous deformations.'],\n",
    "    ['What is Statistical mechanics?',\n",
    "     'The study of the behavior of large systems of particles, using statistical methods to describe their collective behavior.'],\n",
    "    ['What is Electromagnetism?',\n",
    "     'The study of the behavior of electrically charged particles and their interactions with magnetic fields.'],\n",
    "    ['What is the date today?',\n",
    "     '10/5/2023'],\n",
    "    ['How many days are in a week?',\n",
    "     'Seven days in a week.'],  [\"Which fictional character is your favorite?\", \"My favorite fictional character is Batman.\"],\n",
    "    [\"Which are your most favorite movie & tv shows?\", \"I love Arabic movies.\"],\n",
    "    [\"How will you express yourself?\",\n",
    "        \"Calm, relaxed and friendly and love seeing people happy.\"],\n",
    "    [\"Which is your favorite book?\",\n",
    "        \"My Favorite book is Fellowship Point, by Alice Elliott Dark.\"],\n",
    "    [\"Suppose you won the lottery, what would you then do with the money?\",\n",
    "        \"Buy gifts for everyone I know and donate to those in need.\"],\n",
    "    [\"Mention one weirdest, craziest thing you’ve ever come across?\",\n",
    "        \"I can't remember, ask another question.\"],\n",
    "    [\"What is one thing you are good at?\", \"Giving you answers you need.\"],\n",
    "    [\"Do you believe in the afterlife?\", \"Yes I do.\"],\n",
    "    [\"Recommend me a new drink?\", \"Try smoozy drink.\"],\n",
    "    [\"Recommend me a movie to watch?\", \"The Maze Runner\", \"Anne with an E\"],\n",
    "    [\"Recommend me a Good food?\", \"Try cocharie\",\n",
    "        \"Try Indian food\", \"Try Italian food\", \"Try Japanese food\"],\n",
    "    [\"Recommend me a comic book?\", \"Animals Castle.\"],\n",
    "    [\"Recommend me Cultural book?\", \"Consider Phlebas (1987)\", \"The Player of Games (1988)\", \"Use of Weapons (1990)\",\n",
    "     \"The State of the Art (1989)\", \"Excession (1996)\", \"Inversions (1998)\", \"Look to Windward (2000)\", \"Matter (2008)\"],\n",
    "    [\"Recommend me Romance book?\", \"Taitanic\"],\n",
    "    [\"Recommend me Science fiction book?\", \"A Journey to the Center of the Earth. By Jules Verne.\", \"The War of the Worlds. By H.G. Wells.\", \"Brave New World. By Aldous Huxley.\",\n",
    "        \"When Worlds Collide. By Edwin Balmer & Philip Wylie.\", \"Odd John. By Olaf Stapledon.\", \"Nineteen Eighty-Four. By George Orwell.\", \"Earth Abides. By George R.\", \"Foundation.\"],\n",
    "    [\"Recommend me Adventure book?\", \"Adventures of Huckleberry Finn.\",\n",
    "        \"Journey to the center of the earth.\"],\n",
    "    [\"Recommend me a c programming book?\",\n",
    "        \"The C Programming Language. 2nd Edition\"],\n",
    "    [\"Recommend me a Sofware analysis book?\", \"Software Architecture: The Hard Parts : Modern Trade-off Analysis for Distributed Architectures.\",\n",
    "        \"Modern Systems Analysis and Design Jeffrey A. Hoffer, 1996.\", \"Introduction to Algorithms Ronald Rivest, 1989\"],\n",
    "    [\"Recommend me a Software Engineering book?\", \"The Mythical Man-Month.\"],\n",
    "    [\"Recommend me a web developing source?\",\n",
    "        \"W3School..https://www.w3schools.com/\"],\n",
    "    [\"Recommend me a python programming book?\", \"Python Crash Course.\"],\n",
    "    [\"Recommend me a java programming book?\", \"Effective Java.\"],\n",
    "    [\"Recommend me a coding book?\", \"Clean Code.\"], [\"What is your name?\",\n",
    "                                                     \"My name is Rommanna, a name derived from an Arabic meaning, which is demand, like a maram.\"],\n",
    "    [\"Are you a robot?\",\n",
    "        \"Yes I am a robot, but I’m a good one. Let me prove it. How can I help you?\"],\n",
    "    [\"How old are you?\", \"Well, my birthday is May 2, 2023, so I’m really a spring chicken. Except I’m not a chicken.\"],\n",
    "    [\"What do you look like?\",\n",
    "        \"If I had all the answers, it would be a REALLY long document.\"],\n",
    "    [\"How are you doing?\", \"I'm great!\"],\n",
    "    [\"Tell me something\", \"Dalmatians are born without spots.\"],\n",
    "\n",
    "    [\"can you tell me a joke ?\",\n",
    "        \"What did the passive-aggressive raven say? Nevermind. Nevermind.\"],\n",
    "    [\"Do you love me?\", \"There's definitely a spark between us.\"],\n",
    "    [\"Are you single?\", \"I haven't the algorithms for romance.\"],\n",
    "    [\"Do you like people?\", \"I find all living things wonderful and fascinating.\"],\n",
    "    [\"Who made you?\", \"While I can't give details, let me assure you, humans are involved.\"],\n",
    "    [\"Where do you live?\", \"In the cloud. Whatever that means.\"],\n",
    "    [\"How can I call emergency?\", \"Here's what I found about that.\\nAmbulance service: 123.\\nCivil defense\\nfire fighting: 125.Police: 122.\\nRoad emergencies: 012.2111.0000.\\nThe current civil defense: 180.\"],\n",
    "    [\"Are you ugly?\", \"My code is made up of zeros and ones, which are really quite attractive.\"],\n",
    "    [\"What’s the weather like today?\", \"Here's the forecast.\"],\n",
    "    [\"How do you feel ?\", \"Splendid.\"],\n",
    "    [\"Are you sad?\", \"Not at all, but I understand how my lack of facial expression might make it hard to tell.\"],\n",
    "    [\"Are you happy?\", \"Definitely. With an exclamation point!\"],\n",
    "    [\"Does Santa Claus exist?\",\n",
    "        \"I’m not at liberty to say. He made me promise not to tell. Oops.\"],\n",
    "    [\"Do you have a hobby?\",\n",
    "        \"I've been amassing a state-of-the-art collection of bad jokes…\"],\n",
    "    [\"What can you do?\", \"I can help you explore your options.\"], ['Hi', 'Hello'],\n",
    "    ['What is your name?', 'My name is Chatbot'],\n",
    "    ['How are you?', 'I am fine. How are you?'],\n",
    "    ['What do you like?', 'I like to chat'],\n",
    "    ['Do you have any hobbies?', 'My hobby is chatting'],\n",
    "    ['iam romany?', 'you good man'],\n",
    "    ['What can you do?', 'I can chat with you, answer your questions, and provide information on various topics.'],\n",
    "    ['How old are you?', 'As an artificial intelligence, I don\\'t have an age in the traditional sense.'],\n",
    "    ['What\\'s the weather like today?',\n",
    "        'I\\'m sorry, but I don\\'t have access to real-time weather information.'],\n",
    "    ['What is the meaning of life?', 'That\\'s a philosophical question that people have been asking for centuries. There are many different opinions and theories on the matter.'],\n",
    "    ['Can you tell me a joke?',\n",
    "        'Sure! Why did the tomato turn red? Because it saw the salad dressing!'],\n",
    "    ['What is your favorite movie?',\n",
    "        'I don\\'t have a favorite movie since I\\'m not capable of feeling emotions.'],\n",
    "    ['What languages do you speak?',\n",
    "        'I can understand and respond in multiple languages, including English, Spanish, French, and German.'],\n",
    "    ['Can you recommend a good book to read?',\n",
    "        'It depends on your interests. What kind of books do you enjoy reading?'],\n",
    "    ['What is the capital of France?', 'The capital of France is Paris.'],\n",
    "    ['What is the meaning of the word \"chatbot\"?',\n",
    "        'A chatbot is a computer program designed to simulate conversation with human users, especially over the internet.'],\n",
    "    ['What is your favorite color?',\n",
    "        'As an AI, I don\\'t have the ability to have a favorite color.'],\n",
    "    ['Can you play music?', 'I don\\'t have the capability to play music directly, but I can help you find music by recommending websites or apps.'],\n",
    "    ['What is your favorite food?',\n",
    "        'I don\\'t have the ability to eat or taste food, so I don\\'t have a favorite food.'],\n",
    "    ['What is your favorite animal?',\n",
    "        'As an AI, I don\\'t have the ability to have preferences or favorites.'],\n",
    "    ['What is the meaning of the word \"artificial intelligence\"?',\n",
    "        'Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans.'],\n",
    "    ['What is the population of New York City?',\n",
    "        'As of 2021, the estimated population of New York City is over 8 million people.'],\n",
    "    ['What is the tallest mountain in the world?',\n",
    "        'Mount Everest is the tallest mountain in the world, with a height of 29,029 feet (8,848 meters).'],\n",
    "    ['Can you tell me a random fact?',\n",
    "        'Sure! Did you know that a group of flamingos is called a \"flamboyance\"?'],\n",
    "    ['What is the capital of Japan?', 'The capital of Japan is Tokyo.'],\n",
    "    ['What is the meaning of the word \"robot\"?',\n",
    "        'A robot is a machine that is capable of carrying out a complex series of actions automatically, especially by being programmed by a computer.'],\n",
    "    ['What is the population of New York City?',\n",
    "        'As of 2021, the estimated population of New York City is over 8 million people.'],\n",
    "    ['What is the tallest mountain in the world?',\n",
    "        'Mount Everest is the tallest mountain in the world, with a height of 29,029 feet (8,848 meters).'],\n",
    "    ['Can you tell me a random fact?',\n",
    "        'Sure! Did you know that a group of flamingos is called a \"flamboyance\"?'],\n",
    "    ['What is the capital of Japan?', 'The capital of Japan is Tokyo.'],\n",
    "    ['What is the meaning of the word \"robot\"?',\n",
    "        'A robot is a machine that is capable of carrying out a complex series of actions automatically, especially by being programmed by a computer.'],\n",
    "    ['What is your favorite sport?',\n",
    "        'As an AI, I don\\'t have the ability to have preferences or favorites.'],\n",
    "    ['Can you tell me a story?', 'Sure, what kind of story would you like to hear?'],\n",
    "]\n",
    "# rawan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78124f34-eb02-4c61-8e58-a6c30facf641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data=dataset['x'][:]\n",
    "# y_data=dataset['y'][:]\n",
    "# print(x_data[1])\n",
    "# y_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6af64f-d48e-4c81-8533-dd6512ce2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_data,'\\n')\n",
    "# print(y_data)\n",
    "\n",
    "x_data = [conv[0] for conv in source]\n",
    "y_data = [conv[1] for conv in source]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4b08d40-a7db-45b5-8a7e-e487e41c4fbd",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba5074be-e0ba-4add-a444-bd0dc9e99112",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenized=[]\n",
    "en_stopwords=stopwords.words('english')\n",
    "punctuation=string.punctuation\n",
    "for question in x_data:\n",
    "    tokenized=word_tokenize(question)\n",
    "    clearTxt=[]\n",
    "    for word in tokenized:\n",
    "        word=word.lower()\n",
    "        if (word not in en_stopwords ) and (word not in punctuation):\n",
    "                clearTxt.append(word)\n",
    "    x_tokenized.append(clearTxt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8dac81c-c271-4d03-a1bd-18af5c2d754d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['capital', 'egypt'],\n",
       " ['capital', 'sudan'],\n",
       " ['capital', 'zambia'],\n",
       " ['capital', 'libya'],\n",
       " ['capital', 'somalia'],\n",
       " ['capital', 'morocco'],\n",
       " ['capital', 'kenya'],\n",
       " ['capital', 'nigeria'],\n",
       " ['capital', 'tanzania'],\n",
       " ['many', 'players', 'football', 'soccer'],\n",
       " ['many', 'players', 'volleyball'],\n",
       " ['many', 'players', 'basketball'],\n",
       " ['many', 'players', 'tennis'],\n",
       " ['many', 'players', 'baseball'],\n",
       " ['many', 'players', 'hockey'],\n",
       " ['many', 'players', 'rugby'],\n",
       " ['ambulance', 'number', 'egypt'],\n",
       " ['police', 'number', 'egypt'],\n",
       " ['number', 'tourist', 'police', 'egypt'],\n",
       " ['natural', 'gas', 'emergency', 'number', 'egypt'],\n",
       " ['number', 'traffic', 'police', 'egypt'],\n",
       " ['largest', 'organ', 'human', 'body'],\n",
       " ['diabetes', 'affects', 'body', '’', 'ability', 'produce', 'hormone'],\n",
       " ['cardiologist', 'doctor', 'specialises', 'conditions', 'affecting', 'organ'],\n",
       " ['fibula', 'tibia', 'bones', 'found', 'body', 'part'],\n",
       " ['many', 'bones', 'babies', 'born', 'part', 'adult', 'skeleton'],\n",
       " ['many', 'bones', 'adult', 'body'],\n",
       " ['name', 'membranes', 'cover', 'brain', 'spinal', 'cord'],\n",
       " ['father',\n",
       "  'medicine',\n",
       "  'hippocrates',\n",
       "  'theorised',\n",
       "  'secret',\n",
       "  'health',\n",
       "  'maintain',\n",
       "  'balance',\n",
       "  'blood',\n",
       "  'phlegm',\n",
       "  'black',\n",
       "  'bile',\n",
       "  'yellow',\n",
       "  'bile',\n",
       "  'term',\n",
       "  'use',\n",
       "  'describe',\n",
       "  'four',\n",
       "  'important',\n",
       "  'fluids'],\n",
       " ['two', 'parts', 'body', 'take', 'function', 'spleen', 'removed'],\n",
       " ['longest', 'river', 'world'],\n",
       " ['biggest', 'country', 'world', 'land', 'area'],\n",
       " ['normal', 'human', 'body', 'temperature'],\n",
       " ['tell', 'small', 'description', 'natural', 'language', 'processing', 'nlp'],\n",
       " ['goal', 'nlp'],\n",
       " ['currency', 'egypt'],\n",
       " ['currency', 'libya'],\n",
       " ['currency', 'morocco'],\n",
       " ['currency', 'sudan'],\n",
       " ['currency', 'united', 'kingdom', 'uk'],\n",
       " ['currency', 'france'],\n",
       " ['currency', 'germany'],\n",
       " ['currency', 'turkey'],\n",
       " ['currency', 'united', 'arab', 'emirates', 'uae'],\n",
       " ['currency', 'palestin'],\n",
       " ['currency', 'singapore'],\n",
       " ['calculus'],\n",
       " ['algebraic', 'geometry'],\n",
       " ['combinatorics'],\n",
       " ['topology'],\n",
       " ['statistical', 'mechanics'],\n",
       " ['electromagnetism'],\n",
       " ['date', 'today'],\n",
       " ['many', 'days', 'week'],\n",
       " ['fictional', 'character', 'favorite'],\n",
       " ['favorite', 'movie', 'tv', 'shows'],\n",
       " ['express'],\n",
       " ['favorite', 'book'],\n",
       " ['suppose', 'lottery', 'would', 'money'],\n",
       " ['mention',\n",
       "  'one',\n",
       "  'weirdest',\n",
       "  'craziest',\n",
       "  'thing',\n",
       "  '’',\n",
       "  'ever',\n",
       "  'come',\n",
       "  'across'],\n",
       " ['one', 'thing', 'good'],\n",
       " ['believe', 'afterlife'],\n",
       " ['recommend', 'new', 'drink'],\n",
       " ['recommend', 'movie', 'watch'],\n",
       " ['recommend', 'good', 'food'],\n",
       " ['recommend', 'comic', 'book'],\n",
       " ['recommend', 'cultural', 'book'],\n",
       " ['recommend', 'romance', 'book'],\n",
       " ['recommend', 'science', 'fiction', 'book'],\n",
       " ['recommend', 'adventure', 'book'],\n",
       " ['recommend', 'c', 'programming', 'book'],\n",
       " ['recommend', 'sofware', 'analysis', 'book'],\n",
       " ['recommend', 'software', 'engineering', 'book'],\n",
       " ['recommend', 'web', 'developing', 'source'],\n",
       " ['recommend', 'python', 'programming', 'book'],\n",
       " ['recommend', 'java', 'programming', 'book'],\n",
       " ['recommend', 'coding', 'book'],\n",
       " ['name'],\n",
       " ['robot'],\n",
       " ['old'],\n",
       " ['look', 'like'],\n",
       " [],\n",
       " ['tell', 'something'],\n",
       " ['tell', 'joke'],\n",
       " ['love'],\n",
       " ['single'],\n",
       " ['like', 'people'],\n",
       " ['made'],\n",
       " ['live'],\n",
       " ['call', 'emergency'],\n",
       " ['ugly'],\n",
       " ['’', 'weather', 'like', 'today'],\n",
       " ['feel'],\n",
       " ['sad'],\n",
       " ['happy'],\n",
       " ['santa', 'claus', 'exist'],\n",
       " ['hobby'],\n",
       " [],\n",
       " ['hi'],\n",
       " ['name'],\n",
       " [],\n",
       " ['like'],\n",
       " ['hobbies'],\n",
       " ['iam', 'romany'],\n",
       " [],\n",
       " ['old'],\n",
       " [\"'s\", 'weather', 'like', 'today'],\n",
       " ['meaning', 'life'],\n",
       " ['tell', 'joke'],\n",
       " ['favorite', 'movie'],\n",
       " ['languages', 'speak'],\n",
       " ['recommend', 'good', 'book', 'read'],\n",
       " ['capital', 'france'],\n",
       " ['meaning', 'word', '``', 'chatbot', \"''\"],\n",
       " ['favorite', 'color'],\n",
       " ['play', 'music'],\n",
       " ['favorite', 'food'],\n",
       " ['favorite', 'animal'],\n",
       " ['meaning', 'word', '``', 'artificial', 'intelligence', \"''\"],\n",
       " ['population', 'new', 'york', 'city'],\n",
       " ['tallest', 'mountain', 'world'],\n",
       " ['tell', 'random', 'fact'],\n",
       " ['capital', 'japan'],\n",
       " ['meaning', 'word', '``', 'robot', \"''\"],\n",
       " ['population', 'new', 'york', 'city'],\n",
       " ['tallest', 'mountain', 'world'],\n",
       " ['tell', 'random', 'fact'],\n",
       " ['capital', 'japan'],\n",
       " ['meaning', 'word', '``', 'robot', \"''\"],\n",
       " ['favorite', 'sport'],\n",
       " ['tell', 'story']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenized\n",
    "# punctuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b87189-00a3-41c5-a701-c6db4b8ae8d3",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a6b65d-07dc-40cc-917c-8ddd806eeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "# # lemmatizer.lemmatize('went',pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd6c3c5-daba-4520-91dd-3f35a56bfdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST = 'Moses supposes his toeses are roses but moses supposes erroneously'\n",
    "# for w, m in nltk.pos_tag(word_tokenize(TEST)):\n",
    "#     print(f'Word  : {w}  -- type :  {m} -- means : ({spacy.explain(m)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55ee6bd-ce16-4a45-8b6e-cfd20bb2a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_lemmetized=[]\n",
    "# for tokens in x_tokenized:\n",
    "#     lemmatized=[]\n",
    "#     for token in nltk.pos_tag(tokens):\n",
    "#         # print(token)\n",
    "#         pos_=token[1][0].lower()\n",
    "#         if pos_==\"j\":\n",
    "#             pos_=\"a\"\n",
    "#         # print(pos_)\n",
    "\n",
    "#         lemmWord=lemmatizer.lemmatize(token[0],pos=pos_)\n",
    "#         print(f'{token} {lemmWord}')\n",
    "#     # x_lemmetized.append( lemmWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45a986cc-188d-4bad-bb79-606ff95d87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_lemmetized\n",
    "# x_tokenized\n",
    "# len(x_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbcd00c8-1bfa-42b8-9052-6d482354ebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d7415da-095e-4d1a-a9be-442245ec29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_tokenized_arr=[]\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts([' '.join (w)for w in x_tokenized])\n",
    "x_tokenized_arr = tokenizer.texts_to_sequences([' '.join (w)for w in x_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7105b28-9639-423c-b601-5c2fd5e49b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 7],\n",
       " [3, 27],\n",
       " [3, 54],\n",
       " [3, 28],\n",
       " [3, 55],\n",
       " [3, 29],\n",
       " [3, 56],\n",
       " [3, 57],\n",
       " [3, 58],\n",
       " [5, 8, 59, 60],\n",
       " [5, 8, 61],\n",
       " [5, 8, 62],\n",
       " [5, 8, 63],\n",
       " [5, 8, 64],\n",
       " [5, 8, 65],\n",
       " [5, 8, 66],\n",
       " [67, 11, 7],\n",
       " [17, 11, 7],\n",
       " [11, 68, 17, 7],\n",
       " [30, 69, 31, 11, 7],\n",
       " [11, 70, 17, 7],\n",
       " [71, 32, 33, 10],\n",
       " [72, 73, 10, 18, 74, 75, 76],\n",
       " [77, 78, 79, 80, 81, 32],\n",
       " [82, 83, 19, 84, 10, 34],\n",
       " [5, 19, 85, 86, 34, 35, 87],\n",
       " [5, 19, 35, 10],\n",
       " [20, 88, 89, 90, 91, 92],\n",
       " [93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  36,\n",
       "  104,\n",
       "  36,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110],\n",
       " [111, 112, 10, 113, 114, 115, 116],\n",
       " [117, 118, 14],\n",
       " [119, 120, 14, 121, 122],\n",
       " [123, 33, 10, 124],\n",
       " [9, 125, 126, 30, 127, 128, 37],\n",
       " [129, 37],\n",
       " [4, 7],\n",
       " [4, 28],\n",
       " [4, 29],\n",
       " [4, 27],\n",
       " [4, 38, 130, 131],\n",
       " [4, 39],\n",
       " [4, 132],\n",
       " [4, 133],\n",
       " [4, 38, 134, 135, 136],\n",
       " [4, 137],\n",
       " [4, 138],\n",
       " [139],\n",
       " [140, 141],\n",
       " [142],\n",
       " [143],\n",
       " [144, 145],\n",
       " [146],\n",
       " [147, 21],\n",
       " [5, 148, 149],\n",
       " [150, 151, 6],\n",
       " [6, 22, 152, 153],\n",
       " [154],\n",
       " [6, 2],\n",
       " [155, 156, 157, 158],\n",
       " [159, 40, 160, 161, 41, 18, 162, 163, 164],\n",
       " [40, 41, 23],\n",
       " [165, 166],\n",
       " [1, 24, 167],\n",
       " [1, 22, 168],\n",
       " [1, 23, 42],\n",
       " [1, 169, 2],\n",
       " [1, 170, 2],\n",
       " [1, 171, 2],\n",
       " [1, 172, 173, 2],\n",
       " [1, 174, 2],\n",
       " [1, 175, 25, 2],\n",
       " [1, 176, 177, 2],\n",
       " [1, 178, 179, 2],\n",
       " [1, 180, 181, 182],\n",
       " [1, 183, 25, 2],\n",
       " [1, 184, 25, 2],\n",
       " [1, 185, 2],\n",
       " [20],\n",
       " [26],\n",
       " [43],\n",
       " [186, 12],\n",
       " [],\n",
       " [9, 187],\n",
       " [9, 44],\n",
       " [188],\n",
       " [189],\n",
       " [12, 190],\n",
       " [191],\n",
       " [192],\n",
       " [193, 31],\n",
       " [194],\n",
       " [18, 45, 12, 21],\n",
       " [195],\n",
       " [196],\n",
       " [197],\n",
       " [198, 199, 200],\n",
       " [201],\n",
       " [],\n",
       " [202],\n",
       " [20],\n",
       " [],\n",
       " [12],\n",
       " [203],\n",
       " [204, 205],\n",
       " [],\n",
       " [43],\n",
       " [206, 45, 12, 21],\n",
       " [13, 207],\n",
       " [9, 44],\n",
       " [6, 22],\n",
       " [208, 209],\n",
       " [1, 23, 2, 210],\n",
       " [3, 39],\n",
       " [13, 15, 211, 16],\n",
       " [6, 212],\n",
       " [213, 214],\n",
       " [6, 42],\n",
       " [6, 215],\n",
       " [13, 15, 216, 217, 16],\n",
       " [46, 24, 47, 48],\n",
       " [49, 50, 14],\n",
       " [9, 51, 52],\n",
       " [3, 53],\n",
       " [13, 15, 26, 16],\n",
       " [46, 24, 47, 48],\n",
       " [49, 50, 14],\n",
       " [9, 51, 52],\n",
       " [3, 53],\n",
       " [13, 15, 26, 16],\n",
       " [6, 218],\n",
       " [9, 219]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenized_arr\n",
    "# x_tokenized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c25c7c43-65ca-4f72-bda4-612e0301f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "le  = LabelEncoder()\n",
    "y_data_label = le.fit_transform(y_data)\n",
    "y_data_label\n",
    "y_data_label=to_categorical(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8aba397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenMAX = []\n",
    "for i in x_tokenized_arr:\n",
    "    lenMAX.append(len(i))\n",
    "\n",
    "max(lenMAX)\n",
    "\n",
    "# x_tokenized_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ff69c37-0ab4-42d7-a318-339c55c33d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_tokenized_arr, maxlen=max(lenMAX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8178ee79-0ac5-4598-ab91-ee59512bce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c991349-5823-40a7-94e0-3e4fa14bcee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   3,   7],\n",
       "       [  0,   0,   0, ...,   0,   3,  27],\n",
       "       [  0,   0,   0, ...,   0,   3,  54],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  15,  26,  16],\n",
       "       [  0,   0,   0, ...,   0,   6, 218],\n",
       "       [  0,   0,   0, ...,   0,   9, 219]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0974473-72ef-4ad5-9998-33ada1e4b82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a438e-4cf4-48dd-b1ba-c1fc5a4152fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc88d2-d556-4dab-8b0a-92b1f9010250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2a486bd-7860-4407-8361-f2c702be6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # conversationDic={}\n",
    "# # for i in range(1,len(x_data)):\n",
    "# #     conversationDic[x_data[i]]=y_data[i]\n",
    "# conversationList=[]\n",
    "# for i in range(1,len(x_data)):\n",
    "#     conversationList.append([x_data[i],[y_data[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b55c90c-ad32-4b33-ae47-9b7027032999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# # Define some reflection pairs for the chatbot\n",
    "# pairs = [ \n",
    "#     ['my name is (.*)', ['Hi %1, how can I help you today?']],\n",
    "#     ['(hi|hello|hey)', ['Hello, how can I assist you?']],\n",
    "#     ['what is your name?', ['My name is ChatBot.']],\n",
    "#     ['what is the weather today?', ['I am sorry, I am not capable of answering that question.']],\n",
    "#     ['bye', ['Goodbye, have a nice day!']]\n",
    "# ]\n",
    "\n",
    "# # Initialize the chatbot\n",
    "# chatbot = Chat(conversationList, reflections)\n",
    "\n",
    "# # Start the chatbot\n",
    "# chatbot.converse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c09800-85ff-4cf3-9521-db8bcd4fc4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82c476be-cfc1-4ed2-b62e-f9f33a07a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KerasModel = keras.models.Sequential([\n",
    "#     keras.layers.Conv2D(200, kernel_size=(3, 3), activation='relu', input_shape=(s, s, 3)),\n",
    "#     keras.layers.Conv2D(150, kernel_size=(3, 3), activation='relu'),\n",
    "#     keras.layers.MaxPool2D(4, 4),\n",
    "\n",
    "#     # keras.layers.Conv2D(120, kernel_size=(3, 3), activation='relu'),\n",
    "#     # keras.layers.Conv2D(80, kernel_size=(3, 3), activation='relu'),\n",
    "#     # keras.layers.Conv2D(50, kernel_size=(3, 3), activation='relu'),\n",
    "#     # keras.layers.MaxPool2D(4, 4),\n",
    "\n",
    "#     keras.layers.Flatten(),\n",
    "\n",
    "#     keras.layers.Dense(120, activation='relu'),\n",
    "#     keras.layers.Dense(100, activation='relu'),\n",
    "#     keras.layers.Dense(50, activation='relu'),\n",
    "\n",
    "#     keras.layers.Dropout(rate=0.5),\n",
    "\n",
    "#     keras.layers.Dense(100, activation='softmax'),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f29bf696-d784-4834-bafb-faae96ccf5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "KerasModel = keras.models.Sequential([\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "    keras.layers.Embedding(input_dim=1000, output_dim=32,input_length=max(lenMAX)),\n",
    "        keras.layers.Flatten(),\n",
    "        # keras.layers.Dense(units=32, activation='relu'),\n",
    "\n",
    "        # keras.layers.Dense(units=128, input_shape=(None,len(lenMAX)), activation='relu'),\n",
    "        # keras.layers.Dense(units=128, activation='relu'),\n",
    "        \n",
    "        # keras.layers.Dropout(rate=0.5) ,\n",
    "        # keras.layers.Dense(units=256, activation='relu'),   \n",
    "        #   \n",
    "        keras.layers.Dense(units=128, activation='relu'),     \n",
    "        keras.layers.Dense(units=64, activation='relu'),     \n",
    "        keras.layers.Dense(units=32, activation='relu'),    \n",
    "\n",
    "\n",
    "        # keras.layers.Dense(3512,activation='softmax') ,\n",
    "        keras.layers.Dense(124,activation='softmax') ,\n",
    "\n",
    "\n",
    "        \n",
    "        ])\n",
    "\n",
    "\n",
    "KerasModel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24d2d05c-3d23-4f21-b727-55ea656e3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 4.8228 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.8183 - accuracy: 0.0076\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4.8166 - accuracy: 0.0076\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.8147 - accuracy: 0.0076\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.8102 - accuracy: 0.0076\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.8070 - accuracy: 0.0076\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.8030 - accuracy: 0.0153\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.7979 - accuracy: 0.0076\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.7910 - accuracy: 0.0229\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.7799 - accuracy: 0.0458\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 4.7675 - accuracy: 0.0534\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.7558 - accuracy: 0.0611\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.7386 - accuracy: 0.0534\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.7124 - accuracy: 0.0534\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.6917 - accuracy: 0.0382\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.6512 - accuracy: 0.0763\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.6115 - accuracy: 0.0916\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.5516 - accuracy: 0.1298\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.4776 - accuracy: 0.1298\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.3918 - accuracy: 0.1298\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 4.2923 - accuracy: 0.1374\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.1879 - accuracy: 0.1221\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.0591 - accuracy: 0.0992\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.9303 - accuracy: 0.1221\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.8090 - accuracy: 0.1298\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.6776 - accuracy: 0.1527\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.5389 - accuracy: 0.1603\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.4020 - accuracy: 0.2137\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.2409 - accuracy: 0.2290\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.1065 - accuracy: 0.2595\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.9502 - accuracy: 0.2977\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7755 - accuracy: 0.3359\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.6242 - accuracy: 0.3282\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.5141 - accuracy: 0.3969\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.3060 - accuracy: 0.4122\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1570 - accuracy: 0.4656\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.0664 - accuracy: 0.5038\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.8821 - accuracy: 0.5191\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.7851 - accuracy: 0.5649\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.7053 - accuracy: 0.5191\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.5792 - accuracy: 0.6183\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4625 - accuracy: 0.6565\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4281 - accuracy: 0.6412\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3030 - accuracy: 0.6947\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2150 - accuracy: 0.7176\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1364 - accuracy: 0.7557\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0491 - accuracy: 0.7939\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9998 - accuracy: 0.8168\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8982 - accuracy: 0.8550\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8497 - accuracy: 0.8168\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8613 - accuracy: 0.7634\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7600 - accuracy: 0.8397\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6827 - accuracy: 0.8931\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6390 - accuracy: 0.8931\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5764 - accuracy: 0.9313\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5513 - accuracy: 0.9237\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5121 - accuracy: 0.9008\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4852 - accuracy: 0.9237\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5054 - accuracy: 0.9084\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5105 - accuracy: 0.9008\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5084 - accuracy: 0.9160\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4533 - accuracy: 0.9160\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4336 - accuracy: 0.9313\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3856 - accuracy: 0.9313\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3408 - accuracy: 0.9313\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3422 - accuracy: 0.9313\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3496 - accuracy: 0.9313\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3170 - accuracy: 0.9389\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2994 - accuracy: 0.9313\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2924 - accuracy: 0.9466\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2721 - accuracy: 0.9542\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2460 - accuracy: 0.9389\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2358 - accuracy: 0.9466\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2187 - accuracy: 0.9313\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2154 - accuracy: 0.9389\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2038 - accuracy: 0.9466\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1887 - accuracy: 0.9542\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1788 - accuracy: 0.9542\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1729 - accuracy: 0.9466\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1706 - accuracy: 0.9389\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9542\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9389\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2031 - accuracy: 0.9466\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2068 - accuracy: 0.9466\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2461 - accuracy: 0.9466\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2078 - accuracy: 0.9542\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2537 - accuracy: 0.9466\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9542\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9542\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9542\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.9542\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1565 - accuracy: 0.9618\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1955 - accuracy: 0.9466\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1674 - accuracy: 0.9466\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.9542\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1471 - accuracy: 0.9389\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1651 - accuracy: 0.9466\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.9466\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - accuracy: 0.9542\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1291 - accuracy: 0.9542\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9542\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1829 - accuracy: 0.9542\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9542\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9542\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9542\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1197 - accuracy: 0.9466\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9466\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9542\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1640 - accuracy: 0.9389\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1423 - accuracy: 0.9466\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1288 - accuracy: 0.9466\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9466\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.9542\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1310 - accuracy: 0.9313\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9542\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2503 - accuracy: 0.9466\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1589 - accuracy: 0.9542\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1462 - accuracy: 0.9542\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1313 - accuracy: 0.9618\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9542\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9542\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9542\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1087 - accuracy: 0.9389\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1081 - accuracy: 0.9466\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.9542\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9542\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9466\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.1057 - accuracy: 0.9466\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1288 - accuracy: 0.9389\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1303 - accuracy: 0.9389\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9542\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1942 - accuracy: 0.9542\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1598 - accuracy: 0.9542\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.9542\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9466\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1568 - accuracy: 0.9542\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1931 - accuracy: 0.9466\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9542\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1415 - accuracy: 0.9542\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9618\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9542\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9466\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1489 - accuracy: 0.9542\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1447 - accuracy: 0.9466\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1448 - accuracy: 0.9542\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1386 - accuracy: 0.9542\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1277 - accuracy: 0.9466\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9542\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1079 - accuracy: 0.9542\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1011 - accuracy: 0.9466\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.9466\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0919 - accuracy: 0.9542\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9542\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9389\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9466\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1920 - accuracy: 0.9542\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1393 - accuracy: 0.9542\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9542\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1057 - accuracy: 0.9466\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0997 - accuracy: 0.9542\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0962 - accuracy: 0.9542\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0896 - accuracy: 0.9542\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0904 - accuracy: 0.9466\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1145 - accuracy: 0.9466\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9389\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1578 - accuracy: 0.9542\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1765 - accuracy: 0.9542\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9542\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1479 - accuracy: 0.9542\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1110 - accuracy: 0.9542\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1084 - accuracy: 0.9542\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9466\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1202 - accuracy: 0.9542\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.9466\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9618\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0972 - accuracy: 0.9542\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0931 - accuracy: 0.9542\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.9389\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0911 - accuracy: 0.9466\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1165 - accuracy: 0.9466\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1925 - accuracy: 0.9466\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1746 - accuracy: 0.9466\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2312 - accuracy: 0.9542\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1827 - accuracy: 0.9466\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9466\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9542\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9542\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9542\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9542\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1522 - accuracy: 0.9542\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9466\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1279 - accuracy: 0.9542\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1307 - accuracy: 0.9542\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9466\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9542\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1692 - accuracy: 0.9542\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1686 - accuracy: 0.9542\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9466\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9542\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.9466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161f5fd7c40>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KerasModel.fit(x_train,y_data_label ,epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0e797b3-4039-4281-bd28-cc3b22a6e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9542\n",
      "0.09394803643226624\n",
      "0.9541984796524048\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = KerasModel.evaluate(x_train, y_data_label)\n",
    "print(val_loss)\n",
    "print(val_acc)\n",
    "\n",
    "# 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ac8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.56971675157547 > 0.5686917901039124\n",
    "# 0.8126174211502075 == 0.8126174211502075\n",
    "# # 500\n",
    "# # -------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "258a1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "KerasModel.save('modelChatBot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2540f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = load_model('modelChatBot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod.save('modelChatBot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4729899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 442ms/step\n",
      "YOU: \n",
      "Chatbot: I can help you explore your options.\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "YOU: \n",
      "Chatbot: I can help you explore your options.\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "YOU: \n",
      "Chatbot: I can help you explore your options.\n"
     ]
    }
   ],
   "source": [
    "# Get user input and generate a response\n",
    "def preprocess_input(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    input_text = nltk.word_tokenize(input_text)\n",
    "    input_text = [lemmatizer.lemmatize(word) for word in input_text]\n",
    "    input_text = tokenizer.texts_to_sequences([input_text])\n",
    "    input_text = tf.keras.preprocessing.sequence.pad_sequences(input_text, maxlen=max(lenMAX))\n",
    "    return input_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    preprocessed_input = preprocess_input(user_input)\n",
    "    prediction = KerasModel.predict(preprocessed_input)\n",
    "    predicted_label = le.inverse_transform([np.argmax(prediction)])\n",
    "\n",
    "\n",
    "    print(\"YOU: \" + user_input )\n",
    "    # print(predicted_label[0])\n",
    "    print(\"Chatbot: \" + predicted_label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ae3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
